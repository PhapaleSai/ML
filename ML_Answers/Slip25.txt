#Slip 25A Write a python program to implement Polynomial Regression for house price dataset. 

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt

# Dataset
X = [[1500],[1800],[2400],[3000],[3500],[4000]]
y = [400000,500000,600000,650000,700000,750000]

# Polynomial features & model
poly = PolynomialFeatures(2)
X_poly = poly.fit_transform(X)
model = LinearRegression().fit(X_poly, y)

# Predict & plot
y_pred = model.predict(X_poly)
plt.scatter(X, y); plt.plot(X, y_pred, color='red'); plt.show()

# Prediction
print("Price for 2500 sqft:", model.predict(poly.transform([[2500]]))[0])

------------------------------------------------------------------------------------------------
 ##SLIP 16A,25B
 ##Create a two layered neural network with relu and sigmoid activation function

import numpy as np
 
def sigmoid(x): return 1 / (1 + np.exp(-x))
def sigmoid_derivative(x): return x * (1 - x)
def relu(x): return np.maximum(0, x)
def relu_derivative(x): return (x > 0).astype(float)

# Random dataset (XOR problem)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# Initialize weights
W1, b1 = np.random.randn(2, 3), np.zeros((1, 3))
W2, b2 = np.random.randn(3, 1), np.zeros((1, 1))

# Training with forward and backward propagation
for epoch in range(1000):
    hidden = relu(X.dot(W1) + b1)
    output = sigmoid(hidden.dot(W2) + b2)
    error = y - output
    d_output = error * sigmoid_derivative(output)
    d_hidden = d_output.dot(W2.T) * relu_derivative(hidden)

    W2 += hidden.T.dot(d_output) * 0.1
    b2 += np.sum(d_output, axis=0, keepdims=True) * 0.1
    W1 += X.T.dot(d_hidden) * 0.1
    b1 += np.sum(d_hidden, axis=0, keepdims=True) * 0.1

    if epoch % 100 == 0:
        print(f"Epoch {epoch}, MSE: {np.mean(np.square(error))}")
 
print("\nFinal Predictions:")
print(output)

------------------------------------------------------------------------------------------------